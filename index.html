<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Amruta Pai</title>
  
  <meta name="author" content="Amruta Pai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Amruta Pai</name>
              </p>
              <p>I am a Ph.D. candidate in the Electrical and Computer Engineering Department at Rice University.
              </p>
              <p>
                I am a member of the <a href="https://sh.rice.edu">Scalable Health Labs</a>, where I am advised by <a href="https://ashu.rice.edu">Dr. Ashutosh Sabharwal</a>. I am a part of the NSF <a href="https://pathsup.org">PATHS-UP</a>-ERC research team (Thrust 4) which aims at developing advanced health systems for underserved communities targeted for chronic diseases such as cardiovascular diseases and diabetes. 
              </p>
              <p style="text-align:center">
                <a href="mailto:ap52@rice.edu">Email</a> &nbsp/&nbsp
                <a href="data/Amruta_Pai_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=JyQnKlYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/amruta-pai/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/amruta-pai/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Amruta-GHC.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Amruta-GHC.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests are broadly in digital bio-behavioral sensing and modeling different dimensions of human behavior and physiology through signal processing, machine learning, and data science.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Foodhabits.png" alt="clean-usnob" width="284" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/1424-8220/22/7/2753">
                <papertitle>Food Habits: Insights from Food Diaries via Computational Recurrence Measures</papertitle>
              </a>
              <br>
              <strong>Amruta Pai</strong>, <a href="https://sh.rice.edu">Ashutosh Sabharwal</a>
              <br>
              <em>Sensors</em>, 22(7), 2753, 2022
              <p>Humans are creatures of habit, and hence one would expect habitual components in our diet. However, there is scant research characterizing habitual behavior in food consumption quantitatively. Longitudinal food diaries contributed by app users are a promising resource to study habitual behavior in food selection. We developed computational measures that leverage recurrence in food choices to describe the habitual component.</p>
             
            </td>
          </tr>

          
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/HRVCam.png" alt="clean-usnob" width="284" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/journals/journal-of-biomedical-optics/volume-26/issue-02/022707/HRVCam-robust-camera-based-measurement-of-heart-rate-variability/10.1117/1.JBO.26.2.022707.full?SSO=1&tab=ArticleLink">
                <papertitle>HRVCam: robust camera-based measurement of heart rate variability</papertitle>
              </a>
              <br>
              <strong>Amruta Pai</strong>, <a href="http://computationalimaging.rice.edu/team/ashok-veeraraghavan/">Ashok Veeraraghavan</a>, <a href="https://sh.rice.edu">Ashutosh Sabharwal</a>
              <br>
              <em>J. Biomedical Optics</em>, 26(2), 022707, 2021
	      <a href="http://computationalimaging.rice.edu/databases/camerahrv-2/">Dataset</a>
              <p>We propose an algorithm HRVCam that provides sufficient robustness to low SNR and motion-induced artifacts commonly present in imaging photoplethysmography (iPPG) signals. We capture a new dataset containing 16 participants with diverse skin tones. We demonstrate that HRVCam reduces the error in camera-based HRV metrics significantly (more than 50% reduction) for videos with dark skin and face motion.</p>
             
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/healthsense.png" alt="clean-usnob" width="284" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/10.1145/3300061.3345433">
                <papertitle>HealthSense: Software-defined Mobile-based Clinical Trials</papertitle>
              </a>
              <br>
              <a href="https://aidanreececurtis.com">Aidan Curtis</a>, <strong>Amruta Pai</strong>, <a href="https://scholar.google.com/citations?user=8sY4nMcAAAAJ&hl=en">Jian Cao</a>, <a href="https://sh.rice.edu">Ashutosh Sabharwal</a>
              <br>
              <em>Mobicom</em>, 2019
              <p>HealthSense is an innovative platform for managing, hosting, and conducting large scale longitudinal studies that gather both behavioral and biological data, for novel bio-behavioral studies.</p>
             
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/camerahrv.png" alt="clean-usnob" width="284" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://cpb-us-e1.wpmucdn.com/blogs.rice.edu/dist/e/10194/files/2019/10/CameraHRV_preprint.pdf">
                <papertitle>CameraHRV: Robust measurement of heart rate variability using a camera</papertitle>
              </a>
              <br>
              <strong>Amruta Pai</strong>, <a href="http://computationalimaging.rice.edu/team/ashok-veeraraghavan/">Ashok Veeraraghavan</a>, <a href="https://sh.rice.edu">Ashutosh Sabharwal</a>,
              <br>
              <em>SPIE BIOS</em>, 2018
              <p>We developed a new algorithm, CameraHRV, for robustly extracting HRV even in low SNR such as is common with iPPG recordings</p>
             
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/3DPPG.jpeg" alt="clean-usnob" width="284" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9176065">
                <papertitle>PPG3D: Does 3D head tracking improve camera-based PPG estimation?</papertitle>
              </a>
              <br>
              <a href="https://www.researchgate.net/scientific-contributions/Genki-Nagamatsu-2159873682">Genki Nagamatsu</a>, <a href="https://ewanowara.github.io">Ewa Nowara</a>, <strong>Amruta Pai</strong>, <a href="http://computationalimaging.rice.edu/team/ashok-veeraraghavan/">Ashok Veeraraghavan</a>, <a href="https://scholar.google.co.jp/citations?user=OkKk5rMAAAAJ&hl=ja">Hiroshi Kawasaki</a>,
              <br>
              <em>EMBC</em>, 2020
              <p>We use 3D face tracking to estimate the position of facial landmarks with pixel-level accuracy to improve motion robustness of camera-based vital sign estimation.</p>
             
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Patents</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td width="75%" valign="center">
               <a href="https://patentimages.storage.googleapis.com/9e/62/30/206cf6c0176b5d/US20210117782A1.pdf">
                <papertitle>PPG3D: Does 3D head tracking improve camera-based PPG estimation?</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/siddharthkhullar/">Siddharth Khullar</a>, <a href="https://www.linkedin.com/in/apostoloff/">Nicholas Apostoloff</a>, <strong>Amruta Pai</strong>
              <br>
              <em>US Patent App.</em>, 16/945,695
            </td>
          </tr>
          				
        </tbody></table>
      
      </td>
    </tr>
  </table>
</body>

</html>
